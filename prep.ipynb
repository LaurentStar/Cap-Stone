{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from html.parser import HTMLParser\n",
    "import lxml\n",
    "from lxml.html.clean import Cleaner\n",
    "import re\n",
    "from random import randint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods/Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_me(url):\n",
    "    if randint(0, 11) > 8:\n",
    "        time.sleep(1)\n",
    "        \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',\n",
    "    }\n",
    "    try:\n",
    "        html  = requests.get(url, headers=headers).text\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"html.parser\") # create a new bs4 object from the html data loaded\n",
    "    for script in soup([\"script\", \"style\"]): # remove all javascript and stylesheet code\n",
    "        script.extract()\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('|', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_  = os.path.join('data', 'kaggle_fake_news_train.csv')\n",
    "kaggle_fakenews_df = pd.read_csv(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS safe file path\n",
    "_  = os.path.join('data', 'kaggle_news_category_dataset_v2.json')\n",
    "\n",
    "# Get json data as dictionary\n",
    "_ = [json.loads(line) for line in open(_, 'r')]\n",
    "\n",
    "#create dataframe with list of dictionaries\n",
    "news_catagory_df = pd.DataFrame(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_  = os.path.join('data', 'buzzfeed_2018_fake_news.csv')\n",
    "buzz_feed_fakenews_df = pd.read_csv(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Fakenews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all text to one big string\n",
    "kaggle_fakenews_df['total'] = kaggle_fakenews_df['title'] \n",
    "                                + ' ' \n",
    "                                + kaggle_fakenews_df['author'] \n",
    "                                + ' ' \n",
    "                                + kaggle_fakenews_df['author'] \n",
    "                                + ' '\n",
    "                                + kaggle_fakenews_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words\n",
    "kaggle_fakenews_df['total'].map(lambda x: x.removestopwords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuations\n",
    "kaggle_fakenews_df['total'].map(lambda x: x.removepunctionswords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lower Case the words\n",
    "kaggle_fakenews_df['total'].map(lambda x: x.removepunctionswords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(buzz_feed_fakenews_df.head())\n",
    "display(kaggle_fakenews_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buzz_feed_fakenews_df['text'] = buzz_feed_fakenews_df['url'].map(lambda url: clean_me(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-31 09:57:24.840019 index 0\n",
      "2020-03-31 09:58:20.790937 index 40\n",
      "2020-03-31 09:59:21.588104 index 80\n",
      "2020-03-31 10:00:19.277464 index 120\n",
      "2020-03-31 10:01:08.989772 index 160\n",
      "2020-03-31 10:01:54.621034 index 200\n",
      "2020-03-31 10:02:47.153898 index 240\n",
      "2020-03-31 10:03:41.187112 index 280\n",
      "2020-03-31 10:04:26.560986 index 320\n",
      "2020-03-31 10:05:32.208175 index 360\n",
      "2020-03-31 10:06:25.525219 index 400\n",
      "2020-03-31 10:07:11.687489 index 440\n",
      "2020-03-31 10:08:02.521304 index 480\n",
      "2020-03-31 10:08:43.556386 index 520\n",
      "2020-03-31 10:09:31.945163 index 560\n",
      "2020-03-31 10:10:23.044796 index 600\n",
      "2020-03-31 10:11:03.183166 index 640\n",
      "2020-03-31 10:11:45.684805 index 680\n",
      "2020-03-31 10:12:35.841801 index 720\n",
      "2020-03-31 10:13:27.685095 index 760\n",
      "2020-03-31 10:14:45.112673 index 800\n",
      "2020-03-31 10:15:32.232784 index 840\n",
      "2020-03-31 10:16:12.766792 index 880\n",
      "2020-03-31 10:17:02.197822 index 920\n",
      "2020-03-31 10:18:00.686235 index 960\n",
      "2020-03-31 10:18:58.975632 index 1000\n",
      "2020-03-31 10:20:03.583063 index 1040\n",
      "2020-03-31 10:20:48.886766 index 1080\n",
      "2020-03-31 10:22:44.713961 index 1120\n",
      "2020-03-31 10:23:41.031406 index 1160\n",
      "2020-03-31 10:24:35.052841 index 1200\n",
      "2020-03-31 10:26:34.929058 index 1240\n",
      "2020-03-31 10:27:28.084194 index 1280\n",
      "2020-03-31 10:28:28.918004 index 1320\n",
      "2020-03-31 10:29:25.401289 index 1360\n",
      "2020-03-31 10:30:22.558467 index 1400\n",
      "2020-03-31 10:31:21.521623 index 1440\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from html.parser import HTMLParser\n",
    "import lxml\n",
    "from lxml.html.clean import Cleaner\n",
    "import re\n",
    "from random import randint\n",
    "\n",
    "def clean_me(url):\n",
    "    if randint(0, 11) > 8:\n",
    "        time.sleep(1)\n",
    "        \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',\n",
    "    }\n",
    "    try:\n",
    "        html  = requests.get(url, headers=headers).text\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"html.parser\") # create a new bs4 object from the html data loaded\n",
    "    for script in soup([\"script\", \"style\"]): # remove all javascript and stylesheet code\n",
    "        script.extract()\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('|', ' ')\n",
    "    return text\n",
    "\n",
    "_  = os.path.join('data', 'buzzfeed_2018_fake_news.txt')\n",
    "buzz_feed_fakenews_df = pd.read_csv(_)\n",
    "\n",
    "\n",
    "outF = open(\"data.csv\", \"w\") \n",
    "for index, value in enumerate(buzz_feed_fakenews_df['url']): \n",
    "    \n",
    "    outF.write(clean_me(value)) \n",
    "    outF.write(\"\\n\")\n",
    "    if index%100 == 0:\n",
    "        print(datetime.datetime.now(), 'index', index)\n",
    "        \n",
    "outF.close()        \n",
    "        \n",
    "        \n",
    "buzz_feed_fakenews_df['article'] = pd.Series[tmp]\n",
    "buzz_feed_fakenews_df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "buzz_feed_fakenews_df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.truthandaction.org/obama-trump-failing-to-handle-issues-because-of-racism-mommy-issues/'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buzz_feed_fakenews_df['url'][2756]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
