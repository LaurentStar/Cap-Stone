{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "from html.parser import HTMLParser\n",
    "import lxml\n",
    "from lxml.html.clean import Cleaner\n",
    "import re\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods/Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_me(url):\n",
    "    time.sleep(randint(0, 3))\n",
    "        \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        html  = requests.get(url, headers=headers).text\n",
    "    except:\n",
    "        return 'none'\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"html.parser\") # create a new bs4 object from the html data loaded\n",
    "    for script in soup([\"script\", \"style\"]): # remove all javascript and stylesheet code\n",
    "        script.extract()\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('|', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_  = os.path.join('data', 'clean_kaggle_fakenews_train.pkl')\n",
    "_ = open(_, 'rb')\n",
    "_ = pickle.load(_)\n",
    "kaggle_fakenews_df = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_  = os.path.join('data', 'clean_huff.pkl')\n",
    "_ = open(_, 'rb')\n",
    "_ = pickle.load(_)\n",
    "huff_df = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_  = os.path.join('data', 'clean_buzzfeed_fakenews.pkl')\n",
    "_ = open(_, 'rb')\n",
    "_ = pickle.load(_)\n",
    "buzz_feed_fakenews_df = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join('data', 'nytimes_news_articles.txt')\n",
    "articles = []\n",
    "urls = []\n",
    "index = -1\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    lines =  file.readlines()  \n",
    "    for line in lines:    \n",
    "        if line.find('URL:') != -1:\n",
    "            index += 1\n",
    "            urls.append(line.split()[1])\n",
    "            articles.append('')\n",
    "        elif line != '':\n",
    "            articles[index] = articles[index] + line  + ' '\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "new_york_times_df = pd.DataFrame({'total': articles})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Fakenews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all text to one big string\n",
    "kaggle_fakenews_df['total'] = kaggle_fakenews_df.drop(columns=['label']).values.sum(axis=1)\n",
    "kaggle_fakenews_df.to_pickle(\"data/prep_kaggle_fakenews_train.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huffleton Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "huff_df.drop(columns=['date', 'category'], inplace=True)\n",
    "\n",
    "# Scrape articles from all the urls\n",
    "file_path  = os.path.join('data', 'huff_articles.txt')\n",
    "outF = open(file_path, \"a\", encoding=\"utf-8\") \n",
    "for index, value in enumerate(huff_df['link'][:6000]): \n",
    "    outF.write(clean_me(value)) \n",
    "    outF.write(\"\\n\")\n",
    "    if index%100 == 0:\n",
    "        print(datetime.datetime.now(), 'index', index)     \n",
    "outF.close()      \n",
    "\n",
    "file_path = os.path.join('data', 'huff_articles.txt')\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    _ =  file.readlines()    \n",
    "huff_df['text'] =  pd.Series(_)\n",
    "huff_df.fillna(' ', inplace=True)\n",
    "huff_df['total'] = huff_df.drop(columns=['link']).values.sum(axis=1)\n",
    "huff_df['label'] = 0\n",
    "huff_df.iloc[:2167].to_pickle(\"data/prep_huff.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BuzzFeed Fakenews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape articles from all the urls\n",
    "file_path  = os.path.join('data', 'buzzfeed_fake_articles.txt')\n",
    "outF = open(file_path, \"a\", encoding=\"utf-8\") \n",
    "for index, value in enumerate(buzz_feed_fakenews_df['url']): \n",
    "    outF.write(clean_me(value)) \n",
    "    outF.write(\"\\n\")\n",
    "    if index%100 == 0:\n",
    "        print(datetime.datetime.now(), 'index', index)     \n",
    "outF.close()      \n",
    "\n",
    "# Make a new author column and set it equal to the domain of the url\n",
    "buzz_feed_fakenews_df['author'] = buzz_feed_fakenews_df['url'].map(lambda x: x.split('/')[2])\n",
    "\n",
    "#Make a new text column using all the scrap articles\n",
    "file_path = os.path.join('data', 'buzzfeed_fake_articles.txt')\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    _ =  file.readlines()    \n",
    "buzz_feed_fakenews_df['text'] =  pd.Series(_)\n",
    "\n",
    "#Fill any null values with empty string they're all fake so it doesn't matter\n",
    "buzz_feed_fakenews_df.fillna(' ', inplace=True)\n",
    "\n",
    "#Remove column to prepare to merge with kaggle dataset\n",
    "buzz_feed_fakenews_df.drop(columns=['url', 'fb_engagement', 'published_date', 'category'], inplace=True)\n",
    "\n",
    "#Create a total column a concatatation of everything and a label finally pickle for order\n",
    "buzz_feed_fakenews_df['total'] = buzz_feed_fakenews_df.values.sum(axis=1)\n",
    "buzz_feed_fakenews_df['label'] = 1\n",
    "\n",
    "#Conform to the kaggle dataset column ordering\n",
    "_ = kaggle_fakenews_df.columns\n",
    "buzz_feed_fakenews_df = buzz_feed_fakenews_df.reindex(columns=_)\n",
    "\n",
    "buzz_feed_fakenews_df.to_pickle(\"data/prep_buzzfeed_fakenews.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The articles text isn't in the dataset but the urls are. \n",
    "- I perform automatic webscraping using the urls. I save each article as one long string and place it on one line in the txt file. Each line is one article. I read all lines into a list and make a new column called text with this list of long strings\n",
    "- Every observation in this dataset is a fake news article so I decided to just fill any nulls with empty strings to presevre it\n",
    "- A new column called labels is of value 1\n",
    "- Author was just called the website host due to the difficulty of scraping to get the author name for each website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York Times Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_york_times_df['label'] = 0\n",
    "\n",
    "new_york_times_df['total'] = new_york_times_df['total'].map(lambda x: x.replace('\\n', ''))\n",
    "\n",
    "new_york_times_df.to_pickle(\"data/prep_newyork_times.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newyork times articles were in 2 list as reliable news source. I downloaded another dataset with these articles but it wasn't tricky to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all prep dataframes in a list and combine them\n",
    "tmp = [pickle.load(open(os.path.join('data', 'prep_kaggle_fakenews_train.pkl'), 'rb')),\n",
    "       pickle.load(open(os.path.join('data', 'prep_huff.pkl'), 'rb')),\n",
    "       pickle.load(open(os.path.join('data', 'prep_buzzfeed_fakenews.pkl'), 'rb')),\n",
    "       pickle.load(open(os.path.join('data', 'prep_newyork_times.pkl'), 'rb')),\n",
    "      ]\n",
    "final_df = pd.concat([tmp[0][['total', 'label']], tmp[1][['total', 'label']], tmp[2][['total', 'label']], tmp[3]])\n",
    "final_df.to_pickle(\"data/prep_final_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words\n",
    "kaggle_fakenews_df['total'].map(lambda x: x.removestopwords())\n",
    "\n",
    "#Remove punctuations\n",
    "kaggle_fakenews_df['total'].map(lambda x: x.removepunctionswords())\n",
    "\n",
    "#Lower Case the words\n",
    "kaggle_fakenews_df['total'].map(lambda x: x.removepunctionswords())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6. The New Yorker\n",
    "1. The New York Times\n",
    "2. The Wall Street Journal\n",
    "5. The Economist\n",
    "4. BBC\n",
    "- huffpost\n",
    "3. The Washington Post\n",
    "9. The Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    " headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',\n",
    "    }\n",
    "requests.get('https://www.google.com/', headers=headers)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
