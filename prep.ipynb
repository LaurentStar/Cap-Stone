{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Data process. \n",
    "Outcome: \n",
    "- Features extracted \n",
    "- columns concatanated\n",
    "- Labels marked\n",
    "- Data parsing\n",
    "- ready for vectorization\n",
    "- New python objects or csv files created for easier access\n",
    "\n",
    "# Table of Content\n",
    "- Imports\n",
    "- Load Data\n",
    "- Methods/Global/Constants\n",
    "- kaggle_fakenews_df preparing\n",
    "- kaggle_huff_df preparing\n",
    "- kaggle_new_york_times_df preparing\n",
    "- buzzfeed_fakenews_df preparing\n",
    "- Combined Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "from html.parser import HTMLParser\n",
    "import lxml\n",
    "from lxml.html.clean import Cleaner\n",
    "import re\n",
    "from random import randint\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_  = os.path.join('data', 'clean_kaggle_fakenews_train_df.pkl')\n",
    "with open(_, 'rb') as file:\n",
    "    kaggle_fakenews_prep_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_  = os.path.join('data', 'clean_huff_df.pkl')\n",
    "with open(_, 'rb') as file:\n",
    "    kaggle_huff_prep_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_  = os.path.join('data', 'clean_nytimes_df.pkl')\n",
    "with open(_, 'rb') as file:\n",
    "    kaggle_new_york_times_prep_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_  = os.path.join('data', 'clean_buzzfeed_fakenews_df.pkl')\n",
    "with open(_, 'rb') as file:\n",
    "    buzzfeed_fakenews_prep_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods/Global/Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_me(url):\n",
    "    time.sleep(randint(0, 3))\n",
    "        \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        html  = requests.get(url, headers=headers).text\n",
    "    except:\n",
    "        return 'none'\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"html.parser\") # create a new bs4 object from the html data loaded\n",
    "    for script in soup([\"script\", \"style\"]): # remove all javascript and stylesheet code\n",
    "        script.extract()\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('|', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle_fakenews_df preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all text to one big string\n",
    "kaggle_fakenews_prep_df['total'] = kaggle_fakenews_df.drop(columns=['label']).values.sum(axis=1)\n",
    "#kaggle_fakenews_prep_df.to_pickle(\"data/prep_kaggle_fakenews_train_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle_huff_df preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_huff_prep_df.drop(columns=['date', 'category'], inplace=True)\n",
    "\n",
    "#One time call\n",
    "# Scrape articles from all the links & save/add to txt file\n",
    "# file_path  = os.path.join('data', 'scraped_huff_articles.txt')\n",
    "# outF = open(file_path, \"a\", encoding=\"utf-8\") \n",
    "# for index, value in enumerate(kaggle_huff_prep_df['link']): \n",
    "#     outF.write(clean_me(value)) \n",
    "#     outF.write(\"\\n\")  \n",
    "#     # Progress Confirmation\n",
    "#     if index%10000 == 0:\n",
    "#         print(datetime.datetime.now(), 'index', index)     \n",
    "# outF.close()      \n",
    "\n",
    "# Use the saved articles file to fill i  the dataframe column \"articles\"\n",
    "file_path = os.path.join('data', 'scraped_huff_articles.txt')\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    _ =  file.readlines()   \n",
    "    \n",
    "# Some simple parsing and manipulation\n",
    "kaggle_huff_prep_df['text'] =  pd.Series(_)\n",
    "kaggle_huff_prep_df.fillna(' ', inplace=True)\n",
    "kaggle_huff_prep_df['total'] = kaggle_huff_prep_df.drop(columns=['link']).values.sum(axis=1)\n",
    "kaggle_huff_prep_df['label'] = 0\n",
    "\n",
    "\n",
    "# Due to time constraint only 2167 articles were scrapped.\n",
    "#kaggle_huff_prep_df.iloc[:2167].to_pickle(\"data/prep_huff_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle_new_york_times_df preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_new_york_times_prep_df['label'] = 0\n",
    "\n",
    "kaggle_new_york_times_prep_df['total'] = kaggle_new_york_times_prep_df['total'].map(lambda x: x.replace('\\n', ''))\n",
    "\n",
    "#kaggle_new_york_times_prep_df.to_pickle(\"data/prep_nytimes_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# buzzfeed_fakenews_df preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape articles from all the urls for articles\n",
    "# file_path  = os.path.join('data', 'buzzfeed_fake_articles.txt')\n",
    "# outF = open(file_path, \"a\", encoding=\"utf-8\") \n",
    "# for index, value in enumerate(buzzfeed_fakenews_prep_df['url']): \n",
    "#     outF.write(clean_me(value)) \n",
    "#     outF.write(\"\\n\")\n",
    "#     if index%100 == 0:\n",
    "#         print(datetime.datetime.now(), 'index', index)     \n",
    "# outF.close()      \n",
    "\n",
    "\n",
    "#Make a new text column using all the scrap articles\n",
    "file_path = os.path.join('data', 'buzzfeed_fake_articles.txt')\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    _ =  file.readlines()    \n",
    "    \n",
    "buzz_feed_fakenews_df['text'] =  pd.Series(_) \n",
    "    \n",
    "# Make a new author column and set it equal to the domain of the url\n",
    "buzzfeed_fakenews_prep_df['author'] = buzzfeed_fakenews_prep_df['url'].map(lambda x: x.split('/')[2])\n",
    "\n",
    "#Fill any null values with empty string they're all fake so it doesn't matter\n",
    "buzzfeed_fakenews_prep_df.fillna(' ', inplace=True)\n",
    "\n",
    "#Remove column to prepare to merge with kaggle dataset\n",
    "buzzfeed_fakenews_prep_df.drop(columns=['url', 'fb_engagement', 'published_date', 'category'], inplace=True)\n",
    "\n",
    "#Create a total column a concatatation of everything and a label\n",
    "buzzfeed_fakenews_prep_df['total'] = buzzfeed_fakenews_prep_df.values.sum(axis=1)\n",
    "buzzfeed_fakenews_prep_df['label'] = 1\n",
    "\n",
    "#Conform to the kaggle dataset column ordering\n",
    "_ = kaggle_fakenews_prep_df.columns\n",
    "buzzfeed_fakenews_prep_df = buzzfeed_fakenews_prep_df.reindex(columns=_)\n",
    "\n",
    "\n",
    "#buzz_feed_fakenews_df.to_pickle(\"data/prep_buzzfeed_fakenews.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The articles text isn't in the dataset but the urls are. \n",
    "- I perform automatic webscraping using the urls. I save each article as one long string and place it on one line in the txt file. Each line is one article. I read all lines into a list and make a new column called text with this list of long strings\n",
    "- Every observation in this dataset is a fake news article so I decided to just fill any nulls with empty strings to presevre it\n",
    "- A new column called labels is of value 1\n",
    "- Author was just called the website host due to the difficulty of scraping to get the author name for each website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract prepared dataframes and combine them\n",
    "_ = os.path.join('data', 'prepared_data_frames_1.zip') \n",
    "with ZipFile(_, 'r') as zip: \n",
    "    zip.extractall('data')\n",
    "\n",
    "_ = os.path.join('data', 'prepared_data_frames_2.zip') \n",
    "with ZipFile(_, 'r') as zip: \n",
    "    zip.extractall('data')\n",
    "\n",
    "\n",
    "# Get all prep dataframes in a list and combine them\n",
    "tmp = [pickle.load(open(os.path.join('data', 'prep_kaggle_fakenews_train_df.pkl'), 'rb')),\n",
    "       pickle.load(open(os.path.join('data', 'prep_huff_df.pkl'), 'rb')),\n",
    "       pickle.load(open(os.path.join('data', 'prep_buzzfeed_fakenews_df.pkl'), 'rb')),\n",
    "       pickle.load(open(os.path.join('data', 'prep_nytimes_df.pkl'), 'rb')),\n",
    "      ]\n",
    "\n",
    "final_df = pd.concat([tmp[0][['total', 'label']], tmp[1][['total', 'label']], tmp[2][['total', 'label']], tmp[3]])\n",
    "\n",
    "#Saving the finalized dataframe\n",
    "# final_df.to_pickle(\"data/prep_final_df.pkl\")\n",
    "# _ = os.path.join('data', 'prepared_data_frames_3.zip', 'data') \n",
    "# with ZipFile(_, 'w') as zip:\n",
    "#     zip.write(\"data/prep_final_df.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
