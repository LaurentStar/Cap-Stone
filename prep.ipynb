{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "from html.parser import HTMLParser\n",
    "import lxml\n",
    "from lxml.html.clean import Cleaner\n",
    "import re\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods/Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_me(url):\n",
    "    time.sleep(randint(0, 3))\n",
    "        \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        html  = requests.get(url, headers=headers).text\n",
    "    except:\n",
    "        return 'none'\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"html.parser\") # create a new bs4 object from the html data loaded\n",
    "    for script in soup([\"script\", \"style\"]): # remove all javascript and stylesheet code\n",
    "        script.extract()\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('|', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_  = os.path.join('data', 'clean_kaggle_fakenews_train.pkl')\n",
    "_ = open(_, 'rb')\n",
    "_ = pickle.load(_)\n",
    "kaggle_fakenews_df = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_  = os.path.join('data', 'clean_huff.pkl')\n",
    "_ = open(_, 'rb')\n",
    "_ = pickle.load(_)\n",
    "huff_df = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category        date  \\\n",
       "0  Melissa Jeltsen          CRIME  2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT  2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT  2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT  2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT  2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \n",
       "0  She left her husband. He killed their children...  \n",
       "1                           Of course it has a song.  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  \n",
       "4  The \"Dietland\" actress said using the bags is ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.huffingtonpost.com/entry/amazon-prime-what-to-watch_us_5b044625e4b0c0b8b23ec14f'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huff_df['link'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_  = os.path.join('data', 'clean_buzzfeed_fakenews.pkl')\n",
    "_ = open(_, 'rb')\n",
    "_ = pickle.load(_)\n",
    "buzz_feed_fakenews_df = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join('data', 'nytimes_news_articles.txt')\n",
    "articles = []\n",
    "urls = []\n",
    "index = -1\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    lines =  file.readlines()  \n",
    "    for line in lines:    \n",
    "        if line.find('URL:') != -1:\n",
    "            index += 1\n",
    "            urls.append(line.split()[1])\n",
    "            articles.append('')\n",
    "        elif line != '':\n",
    "            articles[index] = articles[index] + line  + ' '\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "new_york_times_df = pd.DataFrame({'total': articles})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Fakenews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all text to one big string\n",
    "kaggle_fakenews_df['total'] = kaggle_fakenews_df.drop(columns=['label']).values.sum(axis=1)\n",
    "kaggle_fakenews_df.to_pickle(\"data/prep_kaggle_fakenews_train.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huffleton Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "huff_df.drop(columns=['date', 'category'], inplace=True)\n",
    "\n",
    "# Scrape articles from all the urls\n",
    "file_path  = os.path.join('data', 'huff_articles.txt')\n",
    "outF = open(file_path, \"a\", encoding=\"utf-8\") \n",
    "for index, value in enumerate(huff_df['link'][:6000]): \n",
    "    outF.write(clean_me(value)) \n",
    "    outF.write(\"\\n\")\n",
    "    if index%100 == 0:\n",
    "        print(datetime.datetime.now(), 'index', index)     \n",
    "outF.close()      \n",
    "\n",
    "file_path = os.path.join('data', 'huff_articles.txt')\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    _ =  file.readlines()    \n",
    "huff_df['text'] =  pd.Series(_)\n",
    "huff_df.fillna(' ', inplace=True)\n",
    "huff_df['total'] = huff_df.drop(columns=['link']).values.sum(axis=1)\n",
    "huff_df['label'] = 0\n",
    "huff_df.iloc[:2167].to_pickle(\"data/prep_huff.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BuzzFeed Fakenews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape articles from all the urls\n",
    "file_path  = os.path.join('data', 'buzzfeed_fake_articles.txt')\n",
    "outF = open(file_path, \"a\", encoding=\"utf-8\") \n",
    "for index, value in enumerate(buzz_feed_fakenews_df['url']): \n",
    "    outF.write(clean_me(value)) \n",
    "    outF.write(\"\\n\")\n",
    "    if index%100 == 0:\n",
    "        print(datetime.datetime.now(), 'index', index)     \n",
    "outF.close()      \n",
    "\n",
    "# Make a new author column and set it equal to the domain of the url\n",
    "buzz_feed_fakenews_df['author'] = buzz_feed_fakenews_df['url'].map(lambda x: x.split('/')[2])\n",
    "\n",
    "#Make a new text column using all the scrap articles\n",
    "file_path = os.path.join('data', 'buzzfeed_fake_articles.txt')\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    _ =  file.readlines()    \n",
    "buzz_feed_fakenews_df['text'] =  pd.Series(_)\n",
    "\n",
    "#Fill any null values with empty string they're all fake so it doesn't matter\n",
    "buzz_feed_fakenews_df.fillna(' ', inplace=True)\n",
    "\n",
    "#Remove column to prepare to merge with kaggle dataset\n",
    "buzz_feed_fakenews_df.drop(columns=['url', 'fb_engagement', 'published_date', 'category'], inplace=True)\n",
    "\n",
    "#Create a total column a concatatation of everything and a label finally pickle for order\n",
    "buzz_feed_fakenews_df['total'] = buzz_feed_fakenews_df.values.sum(axis=1)\n",
    "buzz_feed_fakenews_df['label'] = 1\n",
    "\n",
    "#Conform to the kaggle dataset column ordering\n",
    "_ = kaggle_fakenews_df.columns\n",
    "buzz_feed_fakenews_df = buzz_feed_fakenews_df.reindex(columns=_)\n",
    "\n",
    "buzz_feed_fakenews_df.to_pickle(\"data/prep_buzzfeed_fakenews.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The articles text isn't in the dataset but the urls are. \n",
    "- I perform automatic webscraping using the urls. I save each article as one long string and place it on one line in the txt file. Each line is one article. I read all lines into a list and make a new column called text with this list of long strings\n",
    "- Every observation in this dataset is a fake news article so I decided to just fill any nulls with empty strings to presevre it\n",
    "- A new column called labels is of value 1\n",
    "- Author was just called the website host due to the difficulty of scraping to get the author name for each website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York Times Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_york_times_df['label'] = 0\n",
    "\n",
    "new_york_times_df['total'] = new_york_times_df['total'].map(lambda x: x.replace('\\n', ''))\n",
    "\n",
    "new_york_times_df.to_pickle(\"data/prep_newyork_times.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newyork times articles were in 2 list as reliable news source. I downloaded another dataset with these articles but it wasn't tricky to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all prep dataframes in a list and combine them\n",
    "tmp = [pickle.load(open(os.path.join('data', 'prep_kaggle_fakenews_train.pkl'), 'rb')),\n",
    "       pickle.load(open(os.path.join('data', 'prep_huff.pkl'), 'rb')),\n",
    "       pickle.load(open(os.path.join('data', 'prep_buzzfeed_fakenews.pkl'), 'rb')),\n",
    "       pickle.load(open(os.path.join('data', 'prep_newyork_times.pkl'), 'rb')),\n",
    "      ]\n",
    "final_df = pd.concat([tmp[0][['total', 'label']], tmp[1][['total', 'label']], tmp[2][['total', 'label']], tmp[3]])\n",
    "final_df.to_pickle(\"data/prep_final_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words\n",
    "kaggle_fakenews_df['total'].map(lambda x: x.removestopwords())\n",
    "\n",
    "#Remove punctuations\n",
    "kaggle_fakenews_df['total'].map(lambda x: x.removepunctionswords())\n",
    "\n",
    "#Lower Case the words\n",
    "kaggle_fakenews_df['total'].map(lambda x: x.removepunctionswords())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6. The New Yorker\n",
    "1. The New York Times\n",
    "2. The Wall Street Journal\n",
    "5. The Economist\n",
    "4. BBC\n",
    "- huffpost\n",
    "3. The Washington Post\n",
    "9. The Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    " headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',\n",
    "    }\n",
    "requests.get('https://www.google.com/', headers=headers)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
